{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUhtOjlQpp3XGArayd9rk9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zaribeh/zaribeh/blob/main/Untitled94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "t4xmwacv7pRo",
        "outputId": "140f8964-e6bc-4e39-c510-7487b84c41ee"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-f4ccaa8cd5c0>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f4ccaa8cd5c0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Print\"hi\"\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "Print\"hi\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Constants\n",
        "rho = 8960  # Density of copper in kg/m^3\n",
        "c_p = 385  # Specific heat capacity of copper in J/(kg*K)\n",
        "k = 401  # Thermal conductivity of copper in W/(m*K)\n",
        "sigma_cu = 5.96e7  # Electrical conductivity of copper in S/m\n",
        "diameter = 0.03  # in meters\n",
        "height = 0.01  # in meters\n",
        "total_time = 30e-6  # Total simulation time in seconds (30 microseconds)\n",
        "Nx = 50  # Number of grid points in the x direction\n",
        "Ny = 50  # Number of grid points in the y direction\n",
        "Nt = 200  # Number of time steps\n",
        "\n",
        "# Spatial and temporal grid\n",
        "x = np.linspace(0, diameter, Nx)\n",
        "y = np.linspace(0, height, Ny)\n",
        "t = np.linspace(0, total_time, Nt)\n",
        "X, Y, T = np.meshgrid(x, y, t, indexing='ij')\n",
        "\n",
        "# Flatten the grids\n",
        "X_flat = X.flatten()\n",
        "Y_flat = Y.flatten()\n",
        "T_flat = T.flatten()\n",
        "\n",
        "# Initial conditions\n",
        "T_initial = 300 * np.ones_like(X_flat)  # Initial temperature in K\n",
        "V_initial = np.zeros_like(X_flat)  # Initial voltage in V\n",
        "\n",
        "# Placeholder for boundary conditions\n",
        "q_tot = 1000 * np.ones_like(X_flat)  # Heat flux density in W/m^2\n",
        "j_tot = 10 * np.ones_like(X_flat)  # Electric current density in A/m^2\n",
        "\n",
        "# Define the neural network\n",
        "def create_pinn_model():\n",
        "    inputs = Input(shape=(3,))\n",
        "    x = Dense(50, activation='tanh')(inputs)\n",
        "    for _ in range(7):\n",
        "        x = Dense(50, activation='tanh')(x)\n",
        "    temperature = Dense(1)(x)\n",
        "    voltage = Dense(1)(x)\n",
        "    model = Model(inputs=inputs, outputs=[temperature, voltage])\n",
        "    return model\n",
        "\n",
        "# Create the PINN model\n",
        "model = create_pinn_model()\n",
        "\n",
        "# Define the loss function\n",
        "def pinn_loss(model, x, y, t, T_true, V_true):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        tape.watch([x, y, t])\n",
        "        inputs = tf.stack([x, y, t], axis=1)\n",
        "        T_pred, V_pred = model(inputs)\n",
        "\n",
        "        # Compute gradients for temperature and voltage\n",
        "        T_x = tape.gradient(T_pred, x)\n",
        "        T_y = tape.gradient(T_pred, y)\n",
        "        T_t = tape.gradient(T_pred, t)\n",
        "        V_x = tape.gradient(V_pred, x)\n",
        "        V_y = tape.gradient(V_pred, y)\n",
        "\n",
        "    # Compute second-order derivatives\n",
        "    T_xx = tape.gradient(T_x, x)\n",
        "    T_yy = tape.gradient(T_y, y)\n",
        "    E = j_tot / sigma_cu\n",
        "\n",
        "    # Define the PDE residuals\n",
        "    res_T = T_t - (k * (T_xx + T_yy) + sigma_cu * E**2) / (rho * c_p)\n",
        "    res_V = V_x + V_y  # Simplified for the sake of example\n",
        "\n",
        "    # Define the loss as the mean squared error of the residuals\n",
        "    loss = tf.reduce_mean(tf.square(res_T)) + tf.reduce_mean(tf.square(res_V))\n",
        "    return loss\n",
        "\n",
        "# Compile the model with a custom training loop\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, x, y, t, T_true, V_true):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = pinn_loss(model, x, y, t, T_true, V_true)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# Prepare training data\n",
        "X_train = np.stack([X_flat, Y_flat, T_flat], axis=1)\n",
        "T_train = T_initial.flatten()\n",
        "V_train = V_initial.flatten()\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 1000\n",
        "for epoch in range(n_epochs):\n",
        "    loss = train_step(model, X_flat, Y_flat, T_flat, T_train, V_train)\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.numpy()}')\n",
        "\n",
        "# Prediction\n",
        "T_pred, V_pred = model.predict(X_train)\n",
        "T_pred = T_pred.reshape((Nx, Ny, Nt))\n",
        "V_pred = V_pred.reshape((Nx, Ny, Nt))\n",
        "\n",
        "# Plot the final temperature and voltage distributions\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(T_pred[:, :, -1], cmap='hot', interpolation='nearest', extent=[0, diameter, 0, height], origin='lower')\n",
        "plt.colorbar(label='Temperature (K)')\n",
        "plt.title('Temperature Distribution in the Copper Cylinder after 30 Microseconds')\n",
        "plt.xlabel('x (m)')\n",
        "plt.ylabel('y (m)')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(V_pred[:, :, -1], cmap='coolwarm', interpolation='nearest', extent=[0, diameter, 0, height], origin='lower')\n",
        "plt.colorbar(label='Voltage (V)')\n",
        "plt.title('Voltage Distribution in the Copper Cylinder after 30 Microseconds')\n",
        "plt.xlabel('x (m)')\n",
        "plt.ylabel('y (m)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nEnJAuD7vjq",
        "outputId": "f936774f-8836-4252-9797-2999f7ad9e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_8/bias:0', 'dense_9/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_8/bias:0', 'dense_9/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_8/bias:0', 'dense_9/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_8/bias:0', 'dense_9/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        }
      ]
    }
  ]
}